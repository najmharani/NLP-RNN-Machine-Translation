{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP1MbzYTb5lC+ZZIdc7DALI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"YFXr-oR5--bH"},"outputs":[],"source":["import tensorflow as tf\n","print(tf.__version__)"]},{"cell_type":"code","source":["import pandas as pd\n","\n","df_train = pd.read_parquet(\"hf://datasets/Helsinki-NLP/opus-100/en-id/train-00000-of-00001.parquet\")\n","\n","df_train.head()"],"metadata":{"id":"z48GO2HL_D9N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_of_data = 100000\n","\n","df_train_reduced = df_train.head(num_of_data)\n","df_train_reduced.info()"],"metadata":{"id":"pk1t-I0m_GJP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extract English and Indonesian train texts\n","english_train = df_train_reduced['translation'].apply(lambda x: x['en']).tolist()\n","indonesian_train = df_train_reduced['translation'].apply(lambda x: x['id']).tolist()"],"metadata":{"id":"S7q7qHc3_G2S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","# Tokenization\n","english_tokenizer = Tokenizer()\n","indonesian_tokenizer = Tokenizer()\n","\n","english_tokenizer.fit_on_texts(english_train)\n","indonesian_tokenizer.fit_on_texts(indonesian_train)\n","\n","# Convert texts to sequences\n","english_sequences = english_tokenizer.texts_to_sequences(english_train)\n","indonesian_sequences = indonesian_tokenizer.texts_to_sequences(indonesian_train)\n","\n","english_vocab_size = len(english_tokenizer.word_index) + 1\n","indonesian_vocab_size = len(indonesian_tokenizer.word_index) + 1\n","\n","print(english_vocab_size)\n","print(indonesian_vocab_size)"],"metadata":{"id":"HcOoLhKg_Ivl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_validation = pd.read_parquet(\"hf://datasets/Helsinki-NLP/opus-100/en-id/validation-00000-of-00001.parquet\")\n","\n","df_validation.info()"],"metadata":{"id":"7h2xGXay_KSe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_of_data = 2000\n","\n","df_validation_reduced = df_validation.head(num_of_data)\n","\n","english_validation = df_validation_reduced['translation'].apply(lambda x: x['en']).tolist()\n","indonesian_validation = df_validation_reduced['translation'].apply(lambda x: x['id']).tolist()\n","\n","# Prepare validation data in the same way\n","validation_english_sequences = english_tokenizer.texts_to_sequences(english_validation)\n","validation_indonesian_sequences = indonesian_tokenizer.texts_to_sequences(indonesian_validation)"],"metadata":{"id":"WUy-POFs_Lgw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# Define a consistent maximum sequence length based on the longest sequence in both datasets\n","max_sequence_length = 60\n","\n","# Pad the English and Indonesian sequences for training to the same fixed maximum length\n","english_sequences = pad_sequences(english_sequences, maxlen=max_sequence_length, padding='post')\n","indonesian_sequences = pad_sequences(indonesian_sequences, maxlen=max_sequence_length, padding='post')\n","\n","# Pad the validation sequences to the same maximum sequence length\n","validation_english_sequences = pad_sequences(validation_english_sequences, maxlen=max_sequence_length, padding='post')\n","validation_indonesian_sequences = pad_sequences(validation_indonesian_sequences, maxlen=max_sequence_length, padding='post')\n","\n","# Check the shapes to confirm alignment\n","print(\"Shape of input_sequences:\", english_sequences.shape)\n","print(\"Shape of target_sequences:\", indonesian_sequences.shape)\n","print(\"Shape of validation_english_sequences:\", validation_english_sequences.shape)\n","print(\"Shape of validation_target_sequences:\", validation_indonesian_sequences.shape)\n","print(max_sequence_length)"],"metadata":{"id":"RPKToBg7_NFz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n","\n","# Define the Model\n","embedding_dim = 64\n","\n","model = Sequential([\n","    Embedding(input_dim=english_vocab_size, output_dim=embedding_dim, input_length=max_sequence_length),\n","    SimpleRNN(64, return_sequences=True),\n","    SimpleRNN(128, return_sequences=True),\n","    Dense(indonesian_vocab_size, activation='softmax')\n","])"],"metadata":{"id":"OWnvuc9d_PVb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.callbacks import ModelCheckpoint\n","from tensorflow.keras.callbacks import CSVLogger\n","\n","# Define the path for saving the model and logs\n","path = \"C:/Kuliah/7Sem/NLP/RNN/models-100k/\"\n","\n","checkpoint = ModelCheckpoint(path + 'modelrnn-2_layer-{epoch:02d}.keras', verbose=1, save_best_only=False)\n","csv_logger = CSVLogger(path + \"log-hasil.csv\", append=True, separator=';')"],"metadata":{"id":"IUJarTGl_QaF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.optimizers import Adam\n","\n","model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"],"metadata":{"id":"5nNophqj_SEn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train the Model\n","epochs = 10\n","batch_size = 32\n","\n","history = model.fit(\n","    english_sequences, indonesian_sequences,\n","    epochs=epochs,\n","    batch_size=batch_size,\n","    validation_data=(validation_english_sequences, validation_indonesian_sequences),\n","    callbacks=[checkpoint, csv_logger]\n",")"],"metadata":{"id":"KgEsfyDM_Sfi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","pd.DataFrame(history.history).plot()\n","plt.title(\"Loss\")\n","plt.show()"],"metadata":{"id":"M3GoaRFp_VWe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","\n","def bleu_score(input_data, output_data, input_tokenizer, output_tokenizer, model):\n","    bleu_scores = []\n","    smoothing_function = SmoothingFunction().method1  # Apply smoothing\n","\n","    for i in range(len(input_data)):\n","        # Tokenize and pad single sentence\n","        input_seq = input_tokenizer.texts_to_sequences([input_data[i]])\n","        input_seq = pad_sequences(input_seq, maxlen=max_sequence_length, padding='post')\n","\n","        # Predict for this single input\n","        prediction = model.predict(input_seq)\n","\n","        # Decode the prediction to text\n","        predicted_seq = np.argmax(prediction[0], axis=-1)  # Use only the first (and only) batch element\n","        predicted_text = ' '.join([output_tokenizer.index_word.get(idx, '') for idx in predicted_seq if idx != 0])\n","\n","        # Get the reference text\n","        reference_seq = output_tokenizer.texts_to_sequences([output_data[i]])\n","        reference_seq = pad_sequences(reference_seq, maxlen=max_sequence_length, padding='post')\n","        reference_text = ' '.join([output_tokenizer.index_word.get(idx, '') for idx in reference_seq[0] if idx != 0])\n","\n","        # Tokenize the reference and predicted sentences\n","        reference_tokens = [reference_text.split()]\n","        predicted_tokens = predicted_text.split()\n","\n","        # Calculate BLEU score for this sentence\n","        bleu_score_value = sentence_bleu(reference_tokens, predicted_tokens, smoothing_function=smoothing_function)\n","        bleu_scores.append(bleu_score_value)\n","\n","        # Print predicted and reference text, and BLEU score for the current sentence\n","        print(f\"Sentence {i + 1} Input Text: {input_data[i]}\")\n","        print(f\"Sentence {i + 1} Predicted Text: {predicted_text}\")\n","        print(f\"Sentence {i + 1} Reference Text: {reference_text}\")\n","        print(f\"Sentence {i + 1} BLEU Score: {bleu_score_value:.4f}\")\n","        print(\"-\" * 50)  # Separator for readability\n","\n","    # Calculate the average BLEU score\n","    avg_bleu_score = np.mean(bleu_scores)\n","    print(f\"Average BLEU Score: {avg_bleu_score:.4f}\")\n"],"metadata":{"id":"gaWz-sl8_WoK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_test = pd.read_parquet(\"hf://datasets/Helsinki-NLP/opus-100/en-id/test-00000-of-00001.parquet\")\n","\n","df_test.info()"],"metadata":{"id":"Li7EM1Uo_X4F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_of_data = 20\n","\n","df_test_reduced = df_test.head(num_of_data)\n","df_test_reduced.info()"],"metadata":{"id":"6fljuCQi_ZCi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["english_test = df_test_reduced['translation'].apply(lambda x: x['en']).tolist()\n","indonesian_test = df_test_reduced['translation'].apply(lambda x: x['id']).tolist()\n","\n","print(english_test[:5])\n","print(indonesian_test[:5])"],"metadata":{"id":"TF7rtd9B_aGl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bleu_score(english_test, indonesian_test, english_tokenizer, indonesian_tokenizer, model)"],"metadata":{"id":"c-eMxAsi_bcQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Test the model with a sample input\n","def translate_sentence(input_tokenizer, output_tokenizer, sentence, model):\n","    seq = input_tokenizer.texts_to_sequences([sentence])\n","    seq = pad_sequences(seq, maxlen=max_sequence_length, padding='post')\n","    pred_seq = model.predict(seq)\n","    pred_seq = np.argmax(pred_seq, axis=-1)\n","    translated_words = [output_tokenizer.index_word.get(idx, '') for idx in pred_seq[0] if idx > 0]\n","    return ' '.join(translated_words)"],"metadata":{"id":"TeMd4c9h_c80"},"execution_count":null,"outputs":[]}]}